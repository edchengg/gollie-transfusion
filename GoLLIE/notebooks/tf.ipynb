{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003b73a2",
   "metadata": {},
   "source": [
    "# NER with GoLLIE-TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b015c64",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "\n",
    "See the requeriments.txt file in the main directory to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed51491",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06640c9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import rich\n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004626bb",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "We will load GOLLIE-7B from the huggingface-hub.\n",
    "You can use the function AutoModelForCausalLM.from_pretrained if you prefer it. However, we provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported. We plan to address this in future releases.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93aac182",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25443866",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"ychenNLP/GoLLIE-7B-TF\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b741bc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_input = r'''# The following lines describe the task definition\n",
    "@dataclass\n",
    "class LLM(Entity):\n",
    "    \"\"\"Large language model names or model names. This is used for deep learning and NLP tasks.\"\"\"\n",
    "\n",
    "    span: str  # Such as: \"GPT-3.5\", \"LLama=7B\", \"ChatGPT\"\n",
    "\n",
    "@dataclass\n",
    "class Hyperparams(Entity):\n",
    "    \"\"\"Hyperparameter used for training large language  models. Including learning rate, scheduler, architecture\"\"\"\n",
    "\n",
    "    span: str  # Such as: \"layernorm\", \"cosine scheduler\"\n",
    "\n",
    "# This is the text to analyze\n",
    "text = \"GoLLIE-7B-TFが本日リリースされました！ 1つのNVIDIA A100 GPUで推論が可能なサイズです 学習率は1e-4です 訓練にはLoRAが使用されています\"\n",
    "\n",
    "# This is the English translation of the text\n",
    "eng_text = \"GoLLIE-7B-TF is released today! * Sized for inference on 1 NVIDIA A100 GPUs * learning rate 1e-4 * LoRA is used for training\"\n",
    "\n",
    "# Using translation and fusion\n",
    "# (1) generate annotation for eng_text\n",
    "# (2) generate annotation for text\n",
    "\n",
    "# The annotation instances that take place in the eng_text above are listed here\n",
    "result = [\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54663f7c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model_input = tokenizer(test_input, return_tensors=\"pt\")\n",
    "\n",
    "print(model_input[\"input_ids\"])\n",
    "\n",
    "model_input[\"input_ids\"] = model_input[\"input_ids\"][:, :-1]\n",
    "model_input[\"attention_mask\"] = model_input[\"attention_mask\"][:, :-1]\n",
    "\n",
    "model_ouput = model.generate(\n",
    "    **model_input.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    min_new_tokens=0,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(tokenizer.batch_decode(model_ouput))\n",
    "\n",
    "'''\n",
    "['<s> # The following lines describe the task definition\\n@dataclass\\nclass LLM(Entity):\\n    \"\"\"Large language model names or model names. This is used for deep learning and NLP tasks.\"\"\"\\n\\n    span: str  # Such as: \"GPT-3.5\", \"LLama=7B\", \"ChatGPT\"\\n\\n@dataclass\\nclass Hyperparams(Entity):\\n    \"\"\"Hyperparameter used for training large language  models. Including learning rate, scheduler, architecture\"\"\"\\n\\n    span: str  # Such as: \"layernorm\", \"cosine scheduler\"\\n\\n# This is the text to analyze\\ntext = \"GoLLIE-7B-TFが本日リリースされました！ 1つのNVIDIA A100 GPUで推論が可能なサイズです 学習率は1e-4です 訓練にはLoRAが使用されています\"\\n\\n# This is the English translation of the text\\neng_text = \"GoLLIE-7B-TF is released today! * Sized for inference on 1 NVIDIA A100 GPUs * learning rate 1e-4 * LoRA is used for training\"\\n\\n# Using translation and fusion\\n# (1) generate annotation for eng_text\\n# (2) generate annotation for text\\n\\n# The annotation instances that take place in the eng_text above are listed here\\nresult = [\\n    LLM(span=\"GoLLIE-7B-TF\"),\\n    Hyperparams(span=\"learning rate 1e-4\"),\\n    Hyperparams(span=\"LoRA\"),\\n]\\n\\n# The annotation instances that take place in the text above are listed here\\nfinal_result = [\\n    LLM(span=\"GoLLIE-7B-TF\"),\\n    Hyperparams(span=\"学習率は1e-4\"),\\n    Hyperparams(span=\"LoRA\"),\\n]\\n</s>']\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
