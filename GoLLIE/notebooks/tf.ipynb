{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "003b73a2",
   "metadata": {},
   "source": [
    "# NER with GoLLIE-TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b015c64",
   "metadata": {},
   "source": [
    "### Import requeriments\n",
    "\n",
    "See the requeriments.txt file in the main directory to install the required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed51491",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")  # Add the GoLLIE base directory to sys path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06640c9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import rich\n",
    "import logging\n",
    "from src.model.load_model import load_model\n",
    "import black\n",
    "import inspect\n",
    "from jinja2 import Template\n",
    "import tempfile\n",
    "from src.tasks.utils_typing import AnnotationList\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from typing import Dict, List, Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004626bb",
   "metadata": {},
   "source": [
    "## Load GoLLIE\n",
    "\n",
    "We will load GOLLIE-7B from the huggingface-hub.\n",
    "You can use the function AutoModelForCausalLM.from_pretrained if you prefer it. However, we provide a handy load_model function with many functionalities already implemented that will assist you in reproducing our results.\n",
    "\n",
    "Please note that setting use_flash_attention=True is mandatory. Our flash attention implementation has small numerical differences compared to the attention implementation in Huggingface. Using use_flash_attention=False will result in the model producing inferior results. Flash attention requires an available CUDA GPU. Running GOLLIE pre-trained models on a CPU is not supported. We plan to address this in future releases.\n",
    "\n",
    "- Set force_auto_device_map=True to automatically load the model on available GPUs.\n",
    "- Set quantization=4 if the model doesn't fit in your GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93aac182",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb841c40",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model model from HiTZ/GoLLIE-7B\n",
      "WARNING:root:Using auto device map, we will split the model across GPUs and CPU to fit the model in memory.\n",
      "INFO:root:We will load the model using the following device map: auto and max_memory: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loading model with dtype: torch.bfloat16\n",
      "WARNING:root:Model HiTZ/GoLLIE-7B is an decoder-only model. We will load it as a CausalLM model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Using Flash Attention for LLaMA model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash RoPE installed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [04:42<00:00, 141.33s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.23it/s]\n",
      "INFO:root:Model dtype: torch.bfloat16\n",
      "INFO:root:Total model memory footprint: 13477.101762 MB\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\n",
    "    inference=True,\n",
    "    model_weights_name_or_path=\"ychenNLP/GoLLIE-7B-TF\",\n",
    "    quantization=None,\n",
    "    use_lora=False,\n",
    "    force_auto_device_map=True,\n",
    "    use_flash_attention=True,\n",
    "    torch_dtype=\"bfloat16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b741bc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "test_input = \"# The following lines describe the task definition\\n@dataclass\\nclass Location(Entity):\\n    \\\"\\\"\\\"Roads (streets, motorways) trajectories regions (villages, towns, cities, provinces, countries, continents,\\n    dioceses, parishes) structures (bridges, ports, dams) natural locations (mountains, mountain ranges, woods, rivers,\\n    wells, fields, valleys, gardens, nature reserves, allotments, beaches, national parks) public places (squares, opera\\n    houses, museums, schools, markets, airports, stations, swimming pools, hospitals, sports facilities, youth centers,\\n    parks, town halls, theaters, cinemas, galleries, camping grounds, NASA launch pads, club houses, universities,\\n    libraries, churches, medical centers, parking lots, playgrounds, cemeteries) commercial places (chemists, pubs,\\n    restaurants, depots, hostels, hotels, industrial parks, nightclubs, music venues) assorted buildings (houses, monasteries,\\n    creches, mills, army barracks, castles, retirement homes, towers, halls, rooms, vicarages, courtyards) abstract\\n    ``places'' (e.g. {\\\\it the free world})\\\"\\\"\\\"\\n\\n    span: str  # Such as: \\\"U.S.\\\", \\\"Germany\\\", \\\"Britain\\\", \\\"Australia\\\", \\\"England\\\"\\n\\n\\n@dataclass\\nclass Person(Entity):\\n    \\\"\\\"\\\"first, middle and last names of people, animals and fictional characters aliases.\\\"\\\"\\\"\\n\\n    span: str  # Such as: \\\"Clinton\\\", \\\"Dole\\\", \\\"Arafat\\\", \\\"Yeltsin\\\", \\\"Lebed\\\"\\n\\n\\n@dataclass\\nclass Organization(Entity):\\n    \\\"\\\"\\\"Companies (press agencies, studios, banks, stock markets, manufacturers, cooperatives) subdivisions of\\n    companies (newsrooms) brands political movements (political parties, terrorist organisations) government bodies\\n    (ministries, councils, courts, political unions of countries (e.g. the {\\\\it U.N.})) publications (magazines, newspapers,\\n    journals) musical companies (bands, choirs, opera companies, orchestras public organisations (schools, universities,\\n    charities other collections of people (sports clubs, sports teams, associations, theaters companies, religious orders,\\n    youth organisations.\\\"\\\"\\\"\\n\\n    span: str  # Such as: \\\"Reuters\\\", \\\"U.N.\\\", \\\"NEW YORK\\\", \\\"CHICAGO\\\", \\\"PUK\\\"\\n\\n\\n# This is the text to analyze\\ntext = \\\"Ko min tɔ bɛ kɔ - Dirisa Togola - Minisiriɲɛmɔgɔ , Sogɛli Kokala Mayiga n'a ka Kunnafonidi minisiri Mɛtiri Haruna Ture dalen a kan , taara nin ntɛnɛndon , zuwɛnkalo tile 28 , Kunnafonidalaw ka Soba la .\\\"\\n\\n# This is the English translation of the text\\neng_text = \\\"The remaining party - Dirisa Togola - headed by Prime Minister, Sogeli Kokala Mayiga and his Minister of Information, M. Haruna Ture, attended this Saturday, June 28, at the House of Representatives.\\\"\\n\\n# Using translation and fusion\\n# (1) generate annotation for eng_text\\n# (2) generate annotation for text\\n\\n# The annotation instances that take place in the eng_text above are listed here\\nresult =\"\n",
    "\n",
    "print(test_input)\n",
    "\n",
    "\n",
    "# # The following lines describe the task definition\n",
    "# @dataclass\n",
    "# class Location(Entity):\n",
    "#     \"\"\"Roads (streets, motorways) trajectories regions (villages, towns, cities, provinces, countries, continents,\n",
    "#     dioceses, parishes) structures (bridges, ports, dams) natural locations (mountains, mountain ranges, woods, rivers,\n",
    "#     wells, fields, valleys, gardens, nature reserves, allotments, beaches, national parks) public places (squares, opera\n",
    "#     houses, museums, schools, markets, airports, stations, swimming pools, hospitals, sports facilities, youth centers,\n",
    "#     parks, town halls, theaters, cinemas, galleries, camping grounds, NASA launch pads, club houses, universities,\n",
    "#     libraries, churches, medical centers, parking lots, playgrounds, cemeteries) commercial places (chemists, pubs,\n",
    "#     restaurants, depots, hostels, hotels, industrial parks, nightclubs, music venues) assorted buildings (houses, monasteries,\n",
    "#     creches, mills, army barracks, castles, retirement homes, towers, halls, rooms, vicarages, courtyards) abstract\n",
    "#     ``places'' (e.g. {\\it the free world})\"\"\"\n",
    "\n",
    "#     span: str  # Such as: \"U.S.\", \"Germany\", \"Britain\", \"Australia\", \"England\"\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Person(Entity):\n",
    "#     \"\"\"first, middle and last names of people, animals and fictional characters aliases.\"\"\"\n",
    "\n",
    "#     span: str  # Such as: \"Clinton\", \"Dole\", \"Arafat\", \"Yeltsin\", \"Lebed\"\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Organization(Entity):\n",
    "#     \"\"\"Companies (press agencies, studios, banks, stock markets, manufacturers, cooperatives) subdivisions of\n",
    "#     companies (newsrooms) brands political movements (political parties, terrorist organisations) government bodies\n",
    "#     (ministries, councils, courts, political unions of countries (e.g. the {\\it U.N.})) publications (magazines, newspapers,\n",
    "#     journals) musical companies (bands, choirs, opera companies, orchestras public organisations (schools, universities,\n",
    "#     charities other collections of people (sports clubs, sports teams, associations, theaters companies, religious orders,\n",
    "#     youth organisations.\"\"\"\n",
    "\n",
    "#     span: str  # Such as: \"Reuters\", \"U.N.\", \"NEW YORK\", \"CHICAGO\", \"PUK\"\n",
    "\n",
    "\n",
    "# # This is the text to analyze\n",
    "# text = \"Ko min tɔ bɛ kɔ - Dirisa Togola - Minisiriɲɛmɔgɔ , Sogɛli Kokala Mayiga n'a ka Kunnafonidi minisiri Mɛtiri Haruna Ture dalen a kan , taara nin ntɛnɛndon , zuwɛnkalo tile 28 , Kunnafonidalaw ka Soba la .\"\n",
    "\n",
    "# # This is the English translation of the text\n",
    "# eng_text = \"The remaining party - Dirisa Togola - headed by Prime Minister, Sogeli Kokala Mayiga and his Minister of Information, M. Haruna Ture, attended this Saturday, June 28, at the House of Representatives.\"\n",
    "\n",
    "# # Using translation and fusion\n",
    "# # (1) generate annotation for eng_text\n",
    "# # (2) generate annotation for text\n",
    "\n",
    "# # The annotation instances that take place in the eng_text above are listed here\n",
    "# result ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54663f7c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model_input = tokenizer(test_input, return_tensors=\"pt\")\n",
    "\n",
    "print(model_input[\"input_ids\"])\n",
    "\n",
    "model_input[\"input_ids\"] = model_input[\"input_ids\"][:, :-1]\n",
    "model_input[\"attention_mask\"] = model_input[\"attention_mask\"][:, :-1]\n",
    "\n",
    "model_ouput = model.generate(\n",
    "    **model_input.to(model.device),\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    min_new_tokens=0,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")\n",
    "print(tokenizer.batch_decode(model_ouput))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
